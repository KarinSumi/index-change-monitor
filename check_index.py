import requests
import json
import os
import io
from datetime import datetime, timedelta
import feedparser
from bs4 import BeautifulSoup
import pandas as pd # ‡∏¢‡πâ‡∏≤‡∏¢‡∏°‡∏≤ import ‡∏Ç‡πâ‡∏≤‡∏á‡∏ö‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏ß‡∏£‡πå

# ==================== CONFIGURATION ====================

# Header ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏≠‡∏Å Server ‡∏ß‡πà‡∏≤‡πÄ‡∏£‡∏≤‡πÄ‡∏õ‡πá‡∏ô Browser (‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Wikipedia Block)
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

# ==================== NOTIFICATION FUNCTIONS ====================

def send_discord(message, priority="normal"):
    """‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏õ Discord Webhook"""
    webhook_url = os.environ.get('DISCORD_WEBHOOK')
    if not webhook_url:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö DISCORD_WEBHOOK")
        return False
    
    # ‡πÄ‡∏û‡∏¥‡πà‡∏° emoji ‡∏ï‡∏≤‡∏° priority
    if priority == "high":
        message = "üö® **ALERT** " + message
    
    data = {
        "content": message,
        "username": "Index Monitor Bot v2.0",
        "avatar_url": "https://cdn-icons-png.flaticon.com/512/2111/2111615.png"
    }
    
    try:
        response = requests.post(webhook_url, json=data, timeout=10)
        if response.status_code == 204:
            print("‚úÖ ‡∏™‡πà‡∏á Discord ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
            return True
        else:
            print(f"‚ùå ‡∏™‡πà‡∏á Discord ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {response.status_code}")
            return False
    except Exception as e:
        print(f"‚ùå Error sending Discord: {e}")
        return False


# ==================== S&P OFFICIAL SOURCES ====================

def check_sp_press_releases():
    """
    ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏≤‡∏Å S&P Dow Jones Indices Official Press Releases
    ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Official Source)
    """
    print("\nüì∞ [S&P Official] ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Press Releases...")
    
    try:
        # ‡πÉ‡∏ä‡πâ PR Newswire RSS Feed ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö S&P DJI
        url = "https://www.prnewswire.com/rss/news-releases/s-p-dow-jones-indices-list.rss"
        feed = feedparser.parse(url)
        
        recent_changes = []
        keywords = ['s&p 500', 'sp 500', 'will replace', 'will join', 'added to', 'removed from']
        
        # ‡∏ï‡∏£‡∏ß‡∏à entries ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î (7 ‡∏ß‡∏±‡∏ô‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á)
        cutoff_date = datetime.now() - timedelta(days=7)
        
        for entry in feed.entries[:20]:
            title = entry.title.lower()
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏´‡∏≤ keywords ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
            if any(keyword in title for keyword in keywords):
                # Parse ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà
                try:
                    pub_date = datetime(*entry.published_parsed[:6])
                    if pub_date >= cutoff_date:
                        recent_changes.append({
                            'source': 'S&P Official',
                            'title': entry.title,
                            'link': entry.link,
                            'date': pub_date.strftime('%Y-%m-%d'),
                            'confidence': '‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê'
                        })
                except:
                    pass
        
        if recent_changes:
            print(f"  ‚úÖ ‡∏û‡∏ö‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏® {len(recent_changes)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        else:
            print("  ‚ÑπÔ∏è  ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡πÉ‡∏´‡∏°‡πà")
        
        return recent_changes
        
    except Exception as e:
        print(f"  ‚ùå Error: {e}")
        return []


def scrape_sp_announcements():
    """
    Scrape ‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤ S&P DJI Media Center
    ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Official Source)
    """
    print("\nüì∞ [S&P Media Center] ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®...")
    
    try:
        url = "https://www.spglobal.com/spdji/en/media-center/news-announcements/"
        # ‡πÉ‡∏ä‡πâ Global HEADERS
        response = requests.get(url, headers=HEADERS, timeout=15)
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ‡∏´‡∏≤‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö index changes
        announcements = []
        keywords = ['announces changes', 'will replace', 's&p 500', 's&p midcap', 's&p smallcap']
        
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ links ‡∏ó‡∏µ‡πà‡∏°‡∏µ keywords
        for link in soup.find_all('a', href=True):
            text = link.get_text().lower()
            if any(keyword in text for keyword in keywords):
                announcements.append({
                    'source': 'S&P Media Center',
                    'title': link.get_text().strip(),
                    'link': link['href'] if link['href'].startswith('http') else f"https://www.spglobal.com{link['href']}",
                    'confidence': '‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê'
                })
        
        if announcements:
            print(f"  ‚úÖ ‡∏û‡∏ö‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏® {len(announcements[:5])} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
            return announcements[:5]  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î 5 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£
        else:
            print("  ‚ÑπÔ∏è  ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡πÉ‡∏´‡∏°‡πà")
            return []
            
    except Exception as e:
        print(f"  ‚ùå Error: {e}")
        return []


# ==================== NASDAQ OFFICIAL SOURCES ====================

def check_nasdaq_press_releases():
    """
    ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏≤‡∏Å Nasdaq Official Press Releases
    ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Official Source)
    """
    print("\nüì∞ [Nasdaq Official] ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Press Releases...")
    
    try:
        # ‡πÉ‡∏ä‡πâ Nasdaq Investor Relations RSS
        url = "https://ir.nasdaq.com/rss/news-releases/default.aspx"
        feed = feedparser.parse(url)
        
        recent_changes = []
        keywords = ['nasdaq-100', 'nasdaq 100', 'reconstitution', 'added to', 'removed from']
        
        cutoff_date = datetime.now() - timedelta(days=7)
        
        for entry in feed.entries[:20]:
            title = entry.title.lower()
            
            if any(keyword in title for keyword in keywords):
                try:
                    pub_date = datetime(*entry.published_parsed[:6])
                    if pub_date >= cutoff_date:
                        recent_changes.append({
                            'source': 'Nasdaq Official',
                            'title': entry.title,
                            'link': entry.link,
                            'date': pub_date.strftime('%Y-%m-%d'),
                            'confidence': '‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê'
                        })
                except:
                    pass
        
        if recent_changes:
            print(f"  ‚úÖ ‡∏û‡∏ö‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏® {len(recent_changes)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        else:
            print("  ‚ÑπÔ∏è  ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡πÉ‡∏´‡∏°‡πà")
        
        return recent_changes
        
    except Exception as e:
        print(f"  ‚ùå Error: {e}")
        return []


# ==================== WIKIPEDIA (BACKUP) ====================

def fetch_sp500_list():
    """
    ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏∏‡πâ‡∏ô‡πÉ‡∏ô S&P 500 ‡∏à‡∏≤‡∏Å Wikipedia
    ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠: ‚≠ê‚≠ê‚≠ê (‡∏≠‡∏≤‡∏à‡∏•‡πà‡∏≤‡∏ä‡πâ‡∏≤ 1-2 ‡∏ß‡∏±‡∏ô)
    """
    try:
        url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
        # ‡πÉ‡∏™‡πà Headers ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ 0 stocks
        response = requests.get(url, headers=HEADERS, timeout=15)
        response.raise_for_status()
        
        # ‡πÉ‡∏ä‡πâ io.StringIO ‡πÄ‡∏û‡∏∑‡πà‡∏≠ parse HTML
        tables = pd.read_html(io.StringIO(response.text))
        df = tables[0]
        
        symbols = df['Symbol'].tolist()
        print(f"  ‚úÖ Wikipedia S&P 500: {len(symbols)} ‡∏´‡∏∏‡πâ‡∏ô")
        return set(symbols)
    except Exception as e:
        print(f"  ‚ùå Wikipedia S&P 500 Error: {e}")
        return set()


def fetch_nasdaq100_list():
    """
    ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏∏‡πâ‡∏ô‡πÉ‡∏ô Nasdaq-100 ‡∏à‡∏≤‡∏Å Wikipedia
    ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠: ‚≠ê‚≠ê‚≠ê (‡∏≠‡∏≤‡∏à‡∏•‡πà‡∏≤‡∏ä‡πâ‡∏≤ 1-2 ‡∏ß‡∏±‡∏ô)
    """
    try:
        url = "https://en.wikipedia.org/wiki/Nasdaq-100"
        # ‡πÉ‡∏™‡πà Headers ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ 0 stocks
        response = requests.get(url, headers=HEADERS, timeout=15)
        response.raise_for_status()
        
        # ‡πÉ‡∏ä‡πâ io.StringIO ‡πÄ‡∏û‡∏∑‡πà‡∏≠ parse HTML
        tables = pd.read_html(io.StringIO(response.text))
        
        # ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: Nasdaq Index ‡∏ö‡∏ô Wiki ‡∏≠‡∏≤‡∏à‡∏Ç‡∏¢‡∏±‡∏ö‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÑ‡∏î‡πâ (‡∏õ‡∏Å‡∏ï‡∏¥ 4)
        target_table = None
        for table in tables:
            # ‡∏´‡∏≤‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ col ‡∏ä‡∏∑‡πà‡∏≠ Ticker ‡∏´‡∏£‡∏∑‡∏≠ Symbol
            if 'Ticker' in table.columns:
                target_table = table
                break
            elif 'Symbol' in table.columns:
                target_table = table
                break
        
        if target_table is not None:
            if 'Ticker' in target_table.columns:
                symbols = target_table['Ticker'].tolist()
            else:
                symbols = target_table['Symbol'].tolist()
            
            print(f"  ‚úÖ Wikipedia Nasdaq-100: {len(symbols)} ‡∏´‡∏∏‡πâ‡∏ô")
            return set(symbols)
        else:
            print("  ‚ö†Ô∏è Wikipedia Nasdaq-100: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏´‡∏∏‡πâ‡∏ô")
            return set()
            
    except Exception as e:
        print(f"  ‚ùå Wikipedia Nasdaq-100 Error: {e}")
        return set()


# ==================== DATA STORAGE ====================

def load_previous_data():
    """‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤"""
    try:
        with open('previous_data.json', 'r') as f:
            data = json.load(f)
            print("‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
            return data
    except FileNotFoundError:
        print("‚ö†Ô∏è  ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤ (‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å)")
        return {
            'sp500': [],
            'nasdaq100': [],
            'last_check': None,
            'press_releases_checked': []
        }


def save_current_data(data):
    """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà"""
    try:
        data['last_check'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        with open('previous_data.json', 'w') as f:
            json.dump(data, f, indent=2)
        print("‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
    except Exception as e:
        print(f"‚ùå Error saving: {e}")


# ==================== COMPARISON ====================

def compare_and_notify(index_name, previous_set, current_set):
    """‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô"""
    added = current_set - previous_set
    removed = previous_set - current_set
    
    if added or removed:
        message = f"\nüîî **{index_name} ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á!**\n"
        message += f"üìÖ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        message += f"üîç ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: Wikipedia (‚≠ê‚≠ê‚≠ê)\n"
        
        if added:
            message += f"\n‚úÖ **‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡πâ‡∏≤:**\n"
            for symbol in sorted(added):
                message += f"  ‚Ä¢ `{symbol}`\n"
        
        if removed:
            message += f"\n‚ùå **‡∏ñ‡∏≠‡∏î‡∏≠‡∏≠‡∏Å:**\n"
            for symbol in sorted(removed):
                message += f"  ‚Ä¢ `{symbol}`\n"
        
        message += f"\nüìä ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô: {len(current_set)} ‡∏´‡∏∏‡πâ‡∏ô"
        
        print(message)
        send_discord(message, priority="high")
        return True
    else:
        print(f"  ‚úÖ {index_name}: ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á")
        return False


# ==================== MAIN ====================

def main():
    print("=" * 70)
    print("ü§ñ Index Monitor Bot v2.0 - Multi-Source Edition")
    print(f"üìÖ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 70)
    
    # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏≤‡∏Å Official Press Releases
    print("\n" + "="*70)
    print("üì∞ PHASE 1: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö OFFICIAL PRESS RELEASES")
    print("="*70)
    
    all_announcements = []
    
    # S&P Official Sources
    sp_press = check_sp_press_releases()
    all_announcements.extend(sp_press)
    
    sp_media = scrape_sp_announcements()
    all_announcements.extend(sp_media)
    
    # Nasdaq Official Sources
    nasdaq_press = check_nasdaq_press_releases()
    all_announcements.extend(nasdaq_press)
    
    # ‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏ñ‡πâ‡∏≤‡∏û‡∏ö‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡πÉ‡∏´‡∏°‡πà
    if all_announcements:
        message = "üö® **‡∏û‡∏ö‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡πÉ‡∏´‡∏°‡πà‡∏à‡∏≤‡∏Å Official Sources!**\n\n"
        
        for item in all_announcements[:10]:  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î 10 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£
            message += f"**{item['source']}** {item.get('confidence', '')}\n"
            message += f"üìå {item['title']}\n"
            message += f"üîó {item['link']}\n"
            if 'date' in item:
                message += f"üìÖ {item['date']}\n"
            message += "\n"
        
        if len(all_announcements) > 10:
            message += f"_‡πÅ‡∏•‡∏∞‡∏≠‡∏µ‡∏Å {len(all_announcements) - 10} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£..._\n"
        
        send_discord(message, priority="high")
    
    # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏≤‡∏Å Wikipedia (‡∏™‡∏≥‡∏£‡∏≠‡∏á)
    print("\n" + "="*70)
    print("üìä PHASE 2: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö WIKIPEDIA (Backup)")
    print("="*70)
    
    current_sp500 = fetch_sp500_list()
    current_nasdaq100 = fetch_nasdaq100_list()
    
    # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤
    previous_data = load_previous_data()
    previous_sp500 = set(previous_data.get('sp500', []))
    previous_nasdaq100 = set(previous_data.get('nasdaq100', []))
    
    # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å ‡∏´‡∏£‡∏∑‡∏≠ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤ (‡∏Å‡∏£‡∏ì‡∏µ error 0 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏Å‡πà‡∏≠‡∏ô)
    if not previous_sp500 and not previous_nasdaq100:
        print("\nüöÄ ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô")
        save_current_data({
            'sp500': list(current_sp500),
            'nasdaq100': list(current_nasdaq100),
            'press_releases_checked': []
        })
        
        init_msg = (
            f"üöÄ **‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô Index Monitor Bot v2.0**\n\n"
            f"üìä **S&P 500**: {len(current_sp500)} ‡∏´‡∏∏‡πâ‡∏ô\n"
            f"üìä **Nasdaq-100**: {len(current_nasdaq100)} ‡∏´‡∏∏‡πâ‡∏ô\n\n"
            f"‚úÖ Bot ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ó‡∏≥‡∏á‡∏≤‡∏ô!\n"
            f"üîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏à‡∏≤‡∏Å:\n"
            f"  ‚Ä¢ S&P Official Press Releases ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n"
            f"  ‚Ä¢ Nasdaq Official Press Releases ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n"
            f"  ‚Ä¢ Wikipedia (‡∏™‡∏≥‡∏£‡∏≠‡∏á) ‚≠ê‚≠ê‚≠ê"
        )
        send_discord(init_msg)
        return
    
    # ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
    print("\nüîç ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...")
    sp_changed = compare_and_notify("S&P 500", previous_sp500, current_sp500)
    nasdaq_changed = compare_and_notify("Nasdaq-100", previous_nasdaq100, current_nasdaq100)
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
    if sp_changed or nasdaq_changed or all_announcements:
        save_current_data({
            'sp500': list(current_sp500),
            'nasdaq100': list(current_nasdaq100),
            'press_releases_checked': [item['link'] for item in all_announcements]
        })
    
    # ‡∏™‡∏£‡∏∏‡∏õ
    print("\n" + "="*70)
    print("‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö")
    print("="*70)
    
    if not all_announcements and not sp_changed and not nasdaq_changed:
        print("\n‚ú® ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡πÉ‡∏ô‡∏£‡∏≠‡∏ö‡∏ô‡∏µ‡πâ")


if __name__ == "__main__":
    main()
